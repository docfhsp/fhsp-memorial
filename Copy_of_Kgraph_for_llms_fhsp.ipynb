{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOgDS9Rco7ffr7NxbQVtts7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/docfhsp/fhsp-memorial/blob/main/Copy_of_Kgraph_for_llms_fhsp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# 1. Install necessary libraries\n",
        "!pip install -q google-generativeai langchain langchain-google-genai matplotlib networkx\n",
        "!pip install -q langchain-community\n",
        "\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.api_core import client_options as client_options_lib # For potential region setting if needed\n",
        "import warnings\n",
        "\n",
        "# 2. Configure the Gemini API client\n",
        "try:\n",
        "    genai.configure(\n",
        "        api_key=\"AIzaSyCP7xVEYAkjcrjcelMQVT6M6KiFnS-fit4\", # Enclose the API key in quotes here as well\n",
        "        # Optional: Set transport to 'rest' if needed, or specify client options for region\n",
        "        # transport='rest',\n",
        "        # client_options=client_options_lib.ClientOptions(\n",
        "        #     api_endpoint=\"us-central1-aiplatform.googleapis.com\" # Example endpoint\n",
        "        # )\n",
        "    )\n",
        "    print(\"Gemini API configured successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error configuring Gemini API: {e}\")\n",
        "    # Exit or handle the error appropriately if configuration fails\n",
        "    print(\"Please ensure your API key is correct and valid.\")\n",
        "    exit() # Exit if configuration fails\n",
        "\n",
        "# Also set the environment variable for Langchain integration if needed (some Langchain components might look for it)\n",
        "# Also set the environment variable for Langchain integration if needed (some Langchain components might look for it)\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCP7xVEYAkjcrjcelMQVT6M6KiFnS-fit4\" # Changed GEMINI_API_KEY to \"GOOGLE_API_KEY\"# 3. Direct Question using Gemini API\n",
        "question = \"When did Apple announce the Vision Pro?\"\n",
        "\n",
        "try:\n",
        "    # Instantiate the Gemini model\n",
        "    # Corrected model name - using 'gemini-2.0-flash' as a valid option\n",
        "    model_name_native = 'gemini-2.0-flash'\n",
        "    print(f\"Using Gemini model (Native API): {model_name_native}\")\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name=model_name_native,\n",
        "        # Optional: Add safety settings and generation config\n",
        "        # safety_settings=[ ... ],\n",
        "    )\n",
        "\n",
        "    # Explicitly set temperature to 0 for deterministic output\n",
        "    generation_config = genai.types.GenerationConfig(temperature=0.0)\n",
        "    response = model.generate_content(question, generation_config=generation_config)\n",
        "\n",
        "    # Print the response text\n",
        "    print(\"\\n--- Direct Gemini API Response ---\")\n",
        "    # Add basic check for response content\n",
        "    if response.parts:\n",
        "        print(response.text)\n",
        "    else:\n",
        "        print(\"Received an empty response or content was blocked.\")\n",
        "        # Optionally print safety feedback: print(response.prompt_feedback)\n",
        "    print(\"--------------------------------\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during Gemini API call: {e}\")\n",
        "    print(\"Check if the model name is correct and the API key has permissions.\")\n",
        "\n",
        "\n",
        "# 4. Langchain Integration with Gemini\n",
        "\n",
        "# Import necessary Langchain components\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.indexes import GraphIndexCreator\n",
        "from langchain.chains import GraphQAChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.graphs.networkx_graph import KnowledgeTriple # Keep this for graph structure\n",
        "import networkx as nx # Import earlier for use in this block\n",
        "import matplotlib.pyplot as plt # Import earlier for use in this block\n",
        "\n",
        "# Define the text for graph creation\n",
        "text = \"Apple announced the Vision Pro in 2023.\"\n",
        "\n",
        "try:\n",
        "    # Initialize the Langchain LLM with Gemini\n",
        "    # Pass the API key directly for robustness\n",
        "    # Corrected model name - using 'gemini-2.0-flash'\n",
        "    model_name_langchain = 'gemini-2.0-flash'\n",
        "    print(f\"Using Gemini model (Langchain): {model_name_langchain}\")\n",
        "    # Fix: Use the value from os.environ directly\n",
        "    llm = ChatGoogleGenerativeAI(model=model_name_langchain, temperature=0, google_api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "    # Create the graph index using the Gemini LLM\n",
        "    index_creator = GraphIndexCreator(llm=llm)\n",
        "    graph = index_creator.from_text(text)\n",
        "    print(\"\\n--- Graph Triples from Text ---\")\n",
        "    triples_from_text = graph.get_triples()\n",
        "    print(triples_from_text)\n",
        "    print(\"-------------------------------\\n\")\n",
        "\n",
        "    # 5. Visualize the graph from text (using networkx and matplotlib)\n",
        "\n",
        "    # Create graph only if triples were extracted\n",
        "    if triples_from_text:\n",
        "        G = nx.DiGraph()\n",
        "        G.add_edges_from((str(source), str(target), {'relation': str(relation)}) for source, relation, target in triples_from_text) # Ensure nodes/relation are strings\n",
        "\n",
        "        # Check if graph has nodes before attempting to draw\n",
        "        if G.nodes:\n",
        "            # Plot the graph\n",
        "            plt.figure(figsize=(8,5), dpi=150)\n",
        "            pos = nx.spring_layout(G, k=3, seed=0, iterations=50) # Adjusted iterations\n",
        "\n",
        "            nx.draw_networkx_nodes(G, pos, node_size=1500, node_color='skyblue')\n",
        "            nx.draw_networkx_edges(G, pos, edge_color='gray', arrows=True, arrowstyle='-|>', arrowsize=15) # Clearer arrows\n",
        "            nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
        "            edge_labels = nx.get_edge_attributes(G, 'relation')\n",
        "            nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=9, font_color='red')\n",
        "\n",
        "            # Display the plot\n",
        "            plt.title(\"Graph from Text: 'Apple announced the Vision Pro in 2023'\")\n",
        "            plt.axis('off')\n",
        "            plt.tight_layout() # Adjust layout\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Graph created, but contains no nodes to draw.\")\n",
        "    else:\n",
        "        print(\"No triples extracted from the text to draw a graph.\")\n",
        "\n",
        "\n",
        "    # 6. Question Answering over the graph using Langchain and Gemini\n",
        "    # Ensure graph has content before creating chain\n",
        "    if triples_from_text:\n",
        "      chain = GraphQAChain.from_llm(llm, graph=graph, verbose=True) # Use the Gemini llm instance\n",
        "      print(\"\\n--- QA Chain on Text Graph ---\")\n",
        "      try:\n",
        "        result = chain.run(question)\n",
        "        print(f\"Answer: {result}\")\n",
        "      except Exception as e:\n",
        "        print(f\"Error running QA chain on text graph: {e}\")\n",
        "      print(\"------------------------------\\n\")\n",
        "    else:\n",
        "        print(\"Skipping QA Chain on Text Graph as no triples were generated.\")\n",
        "\n",
        "\n",
        "    # 7. Build and Visualize a Manual Knowledge Graph (No LLM changes needed here)\n",
        "\n",
        "    # Knowledge graph triples\n",
        "    kg = [\n",
        "        ('Apple', 'is', 'Company'), ('Apple', 'created', 'iMac'), ('Apple', 'created', 'iPhone'),\n",
        "        ('Apple', 'created', 'Apple Watch'), ('Apple', 'created', 'Vision Pro'),\n",
        "        ('Apple', 'developed', 'macOS'), ('Apple', 'developed', 'iOS'), ('Apple', 'developed', 'watchOS'),\n",
        "        ('Apple', 'is located in', 'USA'), ('Steve Jobs', 'co-founded', 'Apple'),\n",
        "        ('Steve Wozniak', 'co-founded', 'Apple'), ('Tim Cook', 'is the CEO of', 'Apple'),\n",
        "        ('iOS', 'runs on', 'iPhone'), ('macOS', 'runs on', 'iMac'), ('watchOS', 'runs on', 'Apple Watch'),\n",
        "        ('Apple', 'was founded in', '1976'), ('Apple', 'owns', 'App Store'),\n",
        "        ('App Store', 'sells', 'iOS apps'), ('iPhone', 'announced in', '2007'),\n",
        "        ('iMac', 'announced in', '1998'), ('Apple Watch', 'announced in', '2014'),\n",
        "        ('Vision Pro', 'announced in', '2023'),\n",
        "    ]\n",
        "\n",
        "    # Create a new graph instance for the manual KG\n",
        "    # Need an LLM instance even if just creating an empty graph structure via index_creator\n",
        "    manual_graph = index_creator.from_text('') # Start with an empty graph structure\n",
        "    for (node1, relation, node2) in kg:\n",
        "        manual_graph.add_triple(KnowledgeTriple(node1, relation, node2))\n",
        "\n",
        "    # Create directed graph using networkx\n",
        "    G_manual = nx.DiGraph()\n",
        "    for node1, relation, node2 in kg:\n",
        "        G_manual.add_edge(node1, node2, label=relation) # 'label' is used by draw_networkx_edge_labels\n",
        "\n",
        "    # Plot the manual graph\n",
        "    plt.figure(figsize=(18, 18), dpi=150)\n",
        "    pos_manual = nx.spring_layout(G_manual, k=0.9, iterations=50, seed=42) # Slightly adjusted k\n",
        "\n",
        "    nx.draw_networkx_nodes(G_manual, pos_manual, node_size=3500, node_color='lightgreen') # Adjusted size/color\n",
        "    nx.draw_networkx_edges(G_manual, pos_manual, edge_color='gray', edgelist=G_manual.edges(), width=1.5, arrows=True, arrowstyle='-|>', arrowsize=15) # Adjusted color/arrows\n",
        "    nx.draw_networkx_labels(G_manual, pos_manual, font_size=10, font_weight='bold')\n",
        "    edge_labels_manual = nx.get_edge_attributes(G_manual, 'label')\n",
        "    nx.draw_networkx_edge_labels(G_manual, pos_manual, edge_labels=edge_labels_manual, font_size=9, font_color='darkblue') # Adjusted color\n",
        "\n",
        "    # Display the plot\n",
        "    plt.title(\"Manually Defined Knowledge Graph\")\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout() # Adjust layout\n",
        "    plt.show()\n",
        "\n",
        "    # 8. Question Answering over the Manual Knowledge Graph using Langchain and Gemini\n",
        "    chain_manual = GraphQAChain.from_llm(llm, graph=manual_graph, verbose=True) # Use the Gemini llm instance and the manual graph\n",
        "    print(\"\\n--- QA Chain on Manual KG ---\")\n",
        "    try:\n",
        "      result_manual = chain_manual.run(question)\n",
        "      print(f\"Answer: {result_manual}\")\n",
        "    except Exception as e:\n",
        "      print(f\"Error running QA chain on manual graph: {e}\")\n",
        "    print(\"-----------------------------\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    # Catch potential errors in the Langchain/Graph section (e.g., LLM initialization failed)\n",
        "    print(f\"An error occurred in the Langchain/Graph section: {e}\")\n",
        "    print(\"Check API key permissions and Langchain/Gemini compatibility.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "CmpiD-ZWDeUC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}