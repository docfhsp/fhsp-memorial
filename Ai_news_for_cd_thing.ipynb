{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI Search with Local LLM\n",
        "\n",
        "free colab works. Mistral openhermes works and i love teky's balls of steel giving diamond level dataset which makes these openhermes models. Is what i tested with. Works.\n",
        "\n",
        " very easy to edit the code and make this for arxiv or whatever else. good stuff."
      ],
      "metadata": {
        "id": "xcKlbG02Pix_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir\n",
        "\n",
        "%cd /content\n",
        "!apt-get update -qq && apt-get install -y -qq aria2\n",
        "\n",
        "# Download a local large language model, I'm using OpernHermes-2.5-Mistral-7B-16K-GGUF which has a longer context size and has pretty good quality at its size\n",
        "# If you want to use other local models that can easily run on consumer hardware, check out this repo: https://github.com/Troyanovsky/Local-LLM-Comparison-Colab-UI/\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-16k-GGUF/resolve/main/openhermes-2.5-mistral-7b-16k.Q4_K_M.gguf?download=true -d /content/model/ -o openhermes-2.5-mistral-7b-16k.Q4_K_M.gguf"
      ],
      "metadata": {
        "id": "DIJ_lk4bSlt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTKrS7XSOcR7"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import subprocess\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Function for calling search API\n",
        "def get_search_results(search_term, max_retries=2, retry_delay=2):\n",
        "    url = \"https://google.serper.dev/search\"\n",
        "    payload = json.dumps({\"q\": search_term})\n",
        "    headers = {\n",
        "        'X-API-KEY': '<your_api_key>', # get an API Key stuck here\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "            response.raise_for_status()  # Raise an exception for non-2xx status codes\n",
        "            data = response.json()\n",
        "            organic_results = data.get(\"organic\", [])\n",
        "\n",
        "            search_results = []\n",
        "            search_results_str = \"\"\n",
        "            index = 0\n",
        "            for result in organic_results:\n",
        "                title = result.get(\"title\", \"\")\n",
        "                link = result.get(\"link\", \"\")\n",
        "                snippet = result.get(\"snippet\", \"\")\n",
        "                search_results.append({\"title\": title, \"link\": link, \"snippet\": snippet})\n",
        "                formatted_result = f\"index: {index}\\ntitle: {title}\\nlink: {link}\\nsnippet: {snippet}\\n\\n\"\n",
        "                search_results_str += formatted_result\n",
        "                index += 1\n",
        "            return search_results, search_results_str\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            retries += 1\n",
        "            print(f\"Error: {e}. Retrying in {retry_delay} seconds... (Attempt {retries}/{max_retries})\")\n",
        "            time.sleep(retry_delay)\n",
        "\n",
        "    raise Exception(\"Maximum retries exceeded. Failed to retrieve search results.\")\n",
        "\n",
        "\n",
        "def fetch_url_content(url):\n",
        "    # this is the good thinguie that makes the html llm friendly\n",
        "    prefixed_url = f\"https://r.jina.ai/{url}\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        curl_cmd = [\n",
        "            \"curl\",\n",
        "            \"-H\",\n",
        "            \"Accept: text/event-stream\",\n",
        "            prefixed_url,\n",
        "        ]\n",
        "        curl_process = subprocess.Popen(curl_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        stdout, stderr = curl_process.communicate()\n",
        "\n",
        "        if curl_process.returncode == 0:\n",
        "            content = stdout.decode(\"utf-8\")\n",
        "\n",
        "            content_lines = [line for line in content.split(\"\\n\") if line.startswith(\"data: \")]\n",
        "            if content_lines:\n",
        "                content_data = \"\\n\".join(line[6:] for line in content_lines)\n",
        "                try:\n",
        "                    content_value = json.loads(content_data)[\"content\"]\n",
        "                    return content_value\n",
        "                except (ValueError, KeyError):\n",
        "                    pass\n",
        "\n",
        "            return \"\"\n",
        "        else:\n",
        "            error_message = stderr.decode(\"utf-8\")\n",
        "            raise Exception(f\"cURL request failed: {error_message}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mans go local. Ran on my brazilian notebook. Probably works for u\n",
        "\n",
        "from llama_cpp import Llama\n",
        "\n",
        "def load_llama():\n",
        "    llm = Llama(\n",
        "            model_path=\"/content/model/openhermes-2.5-mistral-7b-16k.Q4_K_M.gguf\", # If you're using another model, change the name\n",
        "            chat_format=\"chatml\", # Use the chat_format that matches the model\n",
        "            n_gpu_layers=-1, # Use -1 for all layers on GPU\n",
        "            n_ctx=12288 # Set context size\n",
        "    )\n",
        "    return llm\n",
        "\n",
        "def call_llama(input: str, llm) -> str:\n",
        "    llm = llm\n",
        "    output = llm.create_chat_completion(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You're a helpful assistant.\",\n",
        "            }, # Feel free to modify the prompt to suit your own formatting needs\n",
        "            {\"role\": \"user\", \"content\": input},\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    output_text = output['choices'][0]['message']['content']\n",
        "    return output_text"
      ],
      "metadata": {
        "id": "QBo4bIOnTBsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pick_url(query, search_results_str, search_results, llm):\n",
        "    llm = llm\n",
        "    prompt = f\"Given the following question, which of the following URLs is most likely to contain the answer for it? Reply ONLY the index number. Question: ```{query}``` List: ```{search_results_str}```\"\n",
        "    index = call_llama(prompt, llm)\n",
        "\n",
        "    max_retries = 2\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            index = int(index.strip())\n",
        "            break\n",
        "        except ValueError:\n",
        "            retries += 1\n",
        "            index = call_llama(prompt, llm)\n",
        "\n",
        "    if retries == max_retries:\n",
        "        raise Exception(\"Failed to convert index to a valid integer after multiple retries.\")\n",
        "\n",
        "    try:\n",
        "        return index\n",
        "    except IndexError:\n",
        "        raise Exception(f\"Invalid index {index} for the search results list.\")"
      ],
      "metadata": {
        "id": "EbAvrHrNYDZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_with_ai(user_input):\n",
        "    llm = None\n",
        "\n",
        "    llm = load_llama()\n",
        "\n",
        "    search_term_prompt = f\"Based on the following question, plesae come up with a search term to use in the search engine. Reply the search term only. Quesiton: ```{user_input}```\"\n",
        "    search_term = call_llama(search_term_prompt, llm)\n",
        "    print(f\"Searching: {search_term}\")\n",
        "\n",
        "    # Seach with search API\n",
        "    search_results, search_results_str = get_search_results(search_term)\n",
        "\n",
        "    # Pick the most relevant URL\n",
        "    try:\n",
        "        top_url_index = pick_url(user_input, search_results_str, search_results, llm)\n",
        "    except Exception as e:\n",
        "        print(f\"Error picking URL: {e}\")\n",
        "        return\n",
        "\n",
        "    # Fetch the content from the top URL\n",
        "    try:\n",
        "        top_url = search_results[top_url_index][\"link\"]\n",
        "        top_snippet = search_results[top_url_index][\"snippet\"]\n",
        "        print(f\"Crawling: {top_url}\")\n",
        "        content = fetch_url_content(top_url)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching URL content: {e}\")\n",
        "        del llm\n",
        "        return\n",
        "\n",
        "    # Truncate the content if it's longer than 36864 characters. I'm using a very lazy estimate here. You can count actual tokens instead.\n",
        "    if len(content) > 36864:\n",
        "        content = content[:36864]\n",
        "\n",
        "    # Call LLM with the content and get the answer. This is the shit original prompt. If u actually use it i can make it way better. Or stun. He would make it way better x10\n",
        "    answer_prompt = f\"Answer the question from the given content. Question: ```{user_input}```\\n\\nContent:```From URL: {top_url} Snippet: {top_snippet}\\n{content}```\"\n",
        "    try:\n",
        "        answer = call_llama(answer_prompt, llm)\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling LLM: {e}\")\n",
        "        return"
      ],
      "metadata": {
        "id": "i_75BFf3XVL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = input(\"What is your question? \\n\")\n",
        "answer = search_with_ai(question)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "794h-hEjXpLs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}